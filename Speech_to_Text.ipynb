{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgkcWm2dX4IY"
      },
      "outputs": [],
      "source": [
        "!pip install openai-whisper torch\n",
        "!pip install pydub\n",
        "!pip install pyannote.audio\n",
        "!pip install openai\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceWcXttvkM_X"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import openai\n",
        "import whisper\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.core import Segment\n",
        "from openai.types import ChatModel\n",
        "import time\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ__PSmIMsnS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Start Drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNG3PDQxRBbL"
      },
      "outputs": [],
      "source": [
        "project_dir = '((Google-Drive-Adress))'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLU48V98zsg2"
      },
      "outputs": [],
      "source": [
        "# OpenAI Project Id and Key description\n",
        "api_key = os.getenv(\"-api-key-\")\n",
        "project_id = os.getenv(\"-project-id's-\")\n",
        "\n",
        "api_key = (\"-api-key-\")\n",
        "openai.api_key = api_key\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# File Upload\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Converting audio file to wav format\n",
        "audio = AudioSegment.from_file(file_name)\n",
        "file_extension = os.path.splitext(file_name)[1].lower()\n",
        "if file_extension != '.wav':\n",
        "    audio.export(\"converted_audio.wav\", format=\"wav\")\n",
        "    file_name = \"converted_audio.wav\"\n",
        "\n",
        "print(\"Audio File Identified\")\n",
        "\n",
        "# Text extraction with Whisper model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(\"medium\", device=device)\n",
        "\n",
        "options = whisper.DecodingOptions(language=\"tr\", beam_size=3, best_of=3)\n",
        "result = model.transcribe(file_name, **options.__dict__)\n",
        "text = result[\"text\"]\n",
        "\n",
        "print(\"Text:\", text)\n",
        "\n",
        "# Writing the extracted text to a file\n",
        "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text)\n",
        "\n",
        "print(\"Text file created\")\n",
        "\n",
        "# pyannote.audio pipeline'ını başlatma\n",
        "try:\n",
        "    pipeline = Pipeline.from_pretrained(\"-Hugging-Face-Model-Name-Adress\", use_auth_token=\"-Hugging-Face-token-address-\")\n",
        "    if pipeline is None:\n",
        "        raise ValueError(\"Failed to install pipeline.\")\n",
        "except Exception as e:\n",
        "    print(f\"Pipeline failed to initialize: {e}\")\n",
        "    raise\n",
        "\n",
        "num_speakers = 2  #  Enter the number of speakers here\n",
        "\n",
        "# Speaker Recognition\n",
        "\n",
        "try:\n",
        "    diarization = pipeline(file_name, num_speakers=num_speakers)\n",
        "    if diarization is None:\n",
        "        raise ValueError(\"Diarization operation failed.\")\n",
        "\n",
        "    # Create an empty list to store speaker recognition result\n",
        "    speaker_segments = []\n",
        "\n",
        "    # Print speaker recognition result\n",
        "    for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        speaker_segments.append((segment.start, segment.end, speaker))\n",
        "        print(f\"Beginning={segment.start:.2f}s Finish={segment.end:.2f}s Speaker_{speaker}\")\n",
        "except Exception as e:\n",
        "    print(f\"Speaker recognition failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Dividing transcription segments into smaller pieces\n",
        "def split_segment(segment, chunk_size=0.5):\n",
        "    start = segment['start']\n",
        "    end = segment['end']\n",
        "    text = segment['text']\n",
        "\n",
        "    chunks = []\n",
        "    current_start = start\n",
        "    while current_start < end:\n",
        "        current_end = min(current_start + chunk_size, end)\n",
        "        chunks.append({'start': current_start, 'end': current_end, 'text': text})\n",
        "        current_start += chunk_size\n",
        "    return chunks\n",
        "\n",
        "# Dividing entire transcription segments into smaller pieces\n",
        "small_segments = []\n",
        "for segment in result['segments']:\n",
        "    small_segments.extend(split_segment(segment))\n",
        "\n",
        "final_transcription = []\n",
        "for segment in result['segments']:\n",
        "    start = segment['start']\n",
        "    end = segment['end']\n",
        "    text_segment = segment['text']\n",
        "\n",
        "    for seg_start, seg_end, speaker in speaker_segments:\n",
        "        if seg_start <= start <= seg_end or seg_start <= end <= seg_end:\n",
        "            final_transcription.append(f\"Speaker {speaker}: {text_segment}\")\n",
        "            break\n",
        "\n",
        "# Writing the final transcription to a file\n",
        "with open(\"final_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for line in final_transcription:\n",
        "        file.write(line + '\\n')\n",
        "\n",
        "# Load text as system message\n",
        "with open(\"final_output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    final_transcription_text = file.read()\n",
        "\n",
        "system_message = f\"Transcription of this audio file: {final_transcription_text}\"\n",
        "\n",
        "print(\"Text file and speaker recognition results created\")\n",
        "\n",
        "def chat_with_gpt3(system_message, user_message, model=\"gpt-3.5-turbo\", max_tokens=150, temperature=0.7, top_p=0.9):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except openai.OpenAIError as e:\n",
        "        if \"Rate limit\" in str(e):\n",
        "            print(f\"Rate limit exceeded: {e}\")\n",
        "            time.sleep(60)  # wait 60 seconds\n",
        "            return chat_with_gpt3(system_message, user_message, model, max_tokens, temperature, top_p)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def is_question_relevant(user_message, context):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": context},\n",
        "            {\"role\": \"user\", \"content\": f\"According to this text '{user_message}' Is there an answer to the question?\"}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0,\n",
        "        top_p=1,\n",
        "    )\n",
        "    relevance_check = response.choices[0].message.content.strip()\n",
        "    return \"Yes\" in relevance_check\n",
        "\n",
        "print(\"Chatbot ready to use..\")\n",
        "\n",
        "# Example usage\n",
        "while True:\n",
        "    user_input = input(\"Ask your question: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    if is_question_relevant(user_input, system_message):\n",
        "        response = chat_with_gpt3(system_message, user_input, model=\"gpt-3.5-turbo\")\n",
        "        print(f\"AI: {response}\")\n",
        "    else:\n",
        "        print(\"Sorry, I don't know about this question.\")\n",
        "    time.sleep(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1ep5mDU5aV9jD-tVSOF-w_VWQngvBmcZP",
      "authorship_tag": "ABX9TyOn3KQsLgb8IytGW4bfmVeb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}